{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrPCyPiG6Nq5",
    "outputId": "6a765535-e165-48bd-cfbf-864c73c709e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hilarifan/Downloads\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/hilarifan/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DwiAgViS8XXo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv') \n",
    "TWEETS_DIR = list(data['tweet'])\n",
    "LABELS_DIR = list(data['valence'])\n",
    "EMBEDDINGS_DIR = './glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JD6u9wPEqqzP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "TWEETS_DIR = np.array(TWEETS_DIR)\n",
    "LABELS_DIR = np.array(LABELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Shuffle data \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "PERCENT_DATA_USED = .1\n",
    "X_train, y_train = shuffle(TWEETS_DIR, LABELS_DIR)\n",
    "\n",
    "#TODO: Data preprocessing: only use PERCENT_DATA_USED of the train/test images and labels\n",
    "num_training = int(len(TWEETS_DIR) * PERCENT_DATA_USED)\n",
    "\n",
    "X_train = X_train[:num_training]\n",
    "y_train = y_train[:num_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IaiGOmcjBnBg",
    "outputId": "40914660-c1fb-4602-c5c7-162554565649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -- Loading tweets and labels\n",
      "2 -- Tokenizing the tweets: converting sentences to sequence of words\n",
      "3 -- Padding sequences to ensure samples are the same size\n",
      "4 -- Loading pre-trained word embeddings. This may take a few minutes.\n",
      "5 -- Finding word embeddings for words in our tweets.\n"
     ]
    }
   ],
   "source": [
    "%run -i load_tweet_data.py\n",
    "\n",
    "tweets, tweets_preprocessed, labels, word_index, embedding_matrix = load_tweet_data(X_train, y_train, EMBEDDINGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0nmkRAes8wr",
    "outputId": "791ec90e-9641-41e2-c3fd-c066fb84774f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  (160000, 45)\n",
      "Number of Tweets:  160000\n",
      "Max Tweet Length:  45\n",
      "\n",
      "Labels Size:  (160000,)\n"
     ]
    }
   ],
   "source": [
    "# What is the size of our dataset?\n",
    "\n",
    "print(\"Training Data Size: \", tweets_preprocessed.shape)\n",
    "print(\"Number of Tweets: \", tweets_preprocessed.shape[0])\n",
    "print(\"Max Tweet Length: \", tweets_preprocessed.shape[1])\n",
    "print()\n",
    "print(\"Labels Size: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "af3_J7cQtBs7"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAGF32iutGMe",
    "outputId": "ede9dcea-f9ff-45a8-9781-47de0e7ee5a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-09 22:42:16.153706: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 45, 50)            6929350   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 45, 64)            29440     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 45, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,993,927\n",
      "Trainable params: 64,577\n",
      "Non-trainable params: 6,929,350\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add pre-trained embedding layer \n",
    "# converts word indices to GloVe word embedding vectors as they're fed in\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=tweets_preprocessed.shape[1],\n",
    "                    trainable=False))\n",
    "\n",
    "# At this point, each individual training sample is now a sequence of word embedding vectors\n",
    "\n",
    "######################################\n",
    "# TODO: define the rest of the network!\n",
    "\n",
    "# First LSTM layer (return sequence so that we can feed the output into the 2nd LSTM layer)\n",
    "model.add(LSTM(64, return_sequences = True, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Second LSTM layer \n",
    "# Don't return sequence this time, because we're feeding into a fully-connected layer\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Dense 1\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Dense 2 (final vote)\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "######################################\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "LOSS = 'binary_crossentropy' \n",
    "OPTIMIZER = 'RMSprop' # RMSprop tends to work well for recurrent models\n",
    "\n",
    "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zmy-2Gfa59wq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(tweets_preprocessed, labels, train_size=0.8, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Am003flHxaAd",
    "outputId": "d991afd1-16ab-4f13-af35-d1a956e8d217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 124s 237ms/step - loss: 0.6049 - binary_accuracy: 0.6744 - val_loss: 0.5647 - val_binary_accuracy: 0.7089\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 121s 242ms/step - loss: 0.5513 - binary_accuracy: 0.7217 - val_loss: 0.5283 - val_binary_accuracy: 0.7364\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 101s 203ms/step - loss: 0.5278 - binary_accuracy: 0.7373 - val_loss: 0.5232 - val_binary_accuracy: 0.7385\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 118s 236ms/step - loss: 0.5119 - binary_accuracy: 0.7476 - val_loss: 0.5228 - val_binary_accuracy: 0.7365\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 85s 171ms/step - loss: 0.5016 - binary_accuracy: 0.7548 - val_loss: 0.4943 - val_binary_accuracy: 0.7586\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 77s 154ms/step - loss: 0.4913 - binary_accuracy: 0.7637 - val_loss: 0.4905 - val_binary_accuracy: 0.7633\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 84s 168ms/step - loss: 0.4822 - binary_accuracy: 0.7671 - val_loss: 0.5405 - val_binary_accuracy: 0.7556\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 89s 178ms/step - loss: 0.4746 - binary_accuracy: 0.7729 - val_loss: 0.5026 - val_binary_accuracy: 0.7555\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 60s 119ms/step - loss: 0.4733 - binary_accuracy: 0.7749 - val_loss: 0.4930 - val_binary_accuracy: 0.7585\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 66s 131ms/step - loss: 0.4644 - binary_accuracy: 0.7794 - val_loss: 0.4914 - val_binary_accuracy: 0.7645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc536838850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "TEST_SIZE = 0.5\n",
    "\n",
    "#####################################\n",
    "# TODO: pick number of epochs and batch size\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#####################################\n",
    "\n",
    "model.fit(X_train1, y_train1, \n",
    "          epochs = EPOCHS, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          validation_split = TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Fr76TORgyOxu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 - 14s - loss: 0.4904 - binary_accuracy: 0.7644 - 14s/epoch - 14ms/step\n",
      "Test loss:  0.49042174220085144 \n",
      "Test accuracy:  0.7643749713897705\n"
     ]
    }
   ],
   "source": [
    "#TODO: Evaluate model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test1,  y_test1, verbose=2)\n",
    "print(\"Test loss: \", test_loss, \"\\nTest accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "caisproject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
